{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishant/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from criteria.parse_related_loss import average_lab_color_loss\n",
    "from tqdm import tqdm\n",
    "from mapper.datasets.latents_dataset_inference import LatentsDatasetInference\n",
    "from mapper.options.test_options import TestOptions\n",
    "from mapper.hairclip_mapper import HairCLIPMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(test_opts):\n",
    "\tdevice = 'cuda:0'\n",
    "\tout_path_results = 'results'\n",
    "\tos.makedirs(out_path_results, exist_ok=True)\n",
    "\t# update test options with options used during training\n",
    "\tckpt = torch.load('/home/ishant/Desktop/Computer Vision/textmodulation/pretrained_models/hairclip.pt', map_location='cpu')\n",
    "\topts = ckpt['opts']\n",
    "\topts.update(vars(test_opts))\n",
    "\topts = Namespace(**opts)\n",
    "\tnet = HairCLIPMapper(opts)\n",
    "\tnet.eval()\n",
    "\tnet.cuda()\n",
    "\n",
    "\ttest_latents = torch.load('messi.pt')\n",
    "\tdataset = LatentsDatasetInference(latents=test_latents.cpu(),\n",
    "\t\t\t\t\t\t\t\t\t\t opts=opts)\n",
    "\tdataloader = DataLoader(dataset,\n",
    "\t                        batch_size=opts.test_batch_size,\n",
    "\t                        shuffle=False,\n",
    "\t                        num_workers=int(opts.test_workers),\n",
    "\t                        drop_last=True)\n",
    "\taverage_color_loss = average_lab_color_loss.AvgLabLoss(opts).to(device).eval()\n",
    "\tassert (opts.start_index >= 0) and (opts.end_index <= len(dataset))\n",
    "\tglobal_i = 0\n",
    "\tfor input_batch in tqdm(dataloader):\n",
    "\t\tif global_i not in range(opts.start_index, opts.end_index):\n",
    "\t\t\tif global_i >=opts.end_index:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tglobal_i += 1\n",
    "\t\t\tcontinue\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tw, hairstyle_text_inputs_list, color_text_inputs_list, selected_description_tuple_list, hairstyle_tensor_list, color_tensor_list = input_batch\n",
    "\t\t\tfor i in range(len(selected_description_tuple_list)):\n",
    "\t\t\t\thairstyle_text_inputs = hairstyle_text_inputs_list[i]\n",
    "\t\t\t\tcolor_text_inputs = color_text_inputs_list[i]\n",
    "\t\t\t\tselected_description = selected_description_tuple_list[i][0]\n",
    "\t\t\t\thairstyle_tensor = hairstyle_tensor_list[i]\n",
    "\t\t\t\tcolor_tensor = color_tensor_list[i]\n",
    "\t\t\t\tw = w.cuda().float()\n",
    "\t\t\t\thairstyle_text_inputs = hairstyle_text_inputs.cuda()\n",
    "\t\t\t\tcolor_text_inputs = color_text_inputs.cuda()\n",
    "\t\t\t\thairstyle_tensor = hairstyle_tensor.cuda()\n",
    "\t\t\t\tcolor_tensor = color_tensor.cuda()\n",
    "\t\t\t\tif hairstyle_tensor.shape[1] != 1:\n",
    "\t\t\t\t\thairstyle_tensor_hairmasked = hairstyle_tensor * average_color_loss.gen_hair_mask(hairstyle_tensor)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\thairstyle_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n",
    "\t\t\t\tif color_tensor.shape[1] != 1:\n",
    "\t\t\t\t\tcolor_tensor_hairmasked = color_tensor * average_color_loss.gen_hair_mask(color_tensor)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcolor_tensor_hairmasked = torch.Tensor([0]).unsqueeze(0).cuda()\n",
    "\t\t\t\tresult_batch = run_on_batch(w, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked, net)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif (hairstyle_tensor.shape[1] != 1) and (color_tensor.shape[1] != 1):\n",
    "\t\t\t\t\timg_tensor = torch.cat([hairstyle_tensor, color_tensor], dim = 3)\n",
    "\t\t\t\telif hairstyle_tensor.shape[1] != 1:\n",
    "\t\t\t\t\timg_tensor = hairstyle_tensor\n",
    "\t\t\t\telif color_tensor.shape[1] != 1:\n",
    "\t\t\t\t\timg_tensor = color_tensor\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\timg_tensor = None\n",
    "\n",
    "\t\t\t\tim_path = str(global_i).zfill(5)\n",
    "\t\t\t\tif img_tensor is not None:\n",
    "\t\t\t\t\tif img_tensor.shape[3] == 1024:\n",
    "\t\t\t\t\t\tcouple_output = torch.cat([result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0), img_tensor])\n",
    "\t\t\t\t\telif img_tensor.shape[3] == 2048:\n",
    "\t\t\t\t\t\tcouple_output = torch.cat([result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0), img_tensor[:,:,:,0:1024], img_tensor[:,:,:,1024::]])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcouple_output = torch.cat([result_batch[2][0].unsqueeze(0), result_batch[0][0].unsqueeze(0)])\n",
    "\t\t\t\tprint(couple_output)\n",
    "\t\t\t\t# torchvision.utils.save_image(couple_output, os.path.join(out_path_results, f\"{im_path}-{str(i).zfill(4)}-{selected_description}.jpg\"), normalize=True, range=(-1, 1))\n",
    "\t\t\tglobal_i += 1\n",
    "\t\t\t\n",
    "def run_on_batch(inputs, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked, net):\n",
    "\tw = inputs\n",
    "\twith torch.no_grad():\n",
    "\t\tw_hat = w + 0.1 * net.mapper(w, hairstyle_text_inputs, color_text_inputs, hairstyle_tensor_hairmasked, color_tensor_hairmasked)\n",
    "\t\tx_hat, w_hat = net.decoder([w_hat], input_is_latent=True, return_latents=True, randomize_noise=False, truncation=1)\n",
    "\t\tresult_batch = (x_hat, w_hat)\n",
    "\t\tx, _ = net.decoder([w], input_is_latent=True, randomize_noise=False, truncation=1)\n",
    "\t\tresult_batch = (x_hat, w_hat, x)\n",
    "\treturn result_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
